# Import required libraries for data manipulation, modeling, and visualization
import numpy as np
import pandas as pd
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow.keras import layers, models

# Load the dataset
dataset = pd.read_csv("creditcardfraud-csv/creditcard.csv")

# Standardize the feature columns (excluding the target "Class" column)
scaler = StandardScaler()
X = scaler.fit_transform(dataset.drop("Class", axis=1))  # Features (scaled)
y = dataset["Class"]  # Target variable

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build and train the Autoencoder model
input_dim = X_train.shape[1]  # Number of input features

# Define the encoder - this compresses the data to a lower dimension
encoder = models.Sequential([
    layers.Input(shape=(input_dim,)),  # Input layer with shape matching feature dimension
    layers.Dense(32, activation='relu'),  # Dense layer with 32 units and ReLU activation
    layers.Dense(16, activation='relu')   # Further compression to 16 units
])

# Define the decoder - this reconstructs the data from the compressed latent space
decoder = models.Sequential([
    layers.Input(shape=(16,)),  # Input layer matching encoder output
    layers.Dense(32, activation='relu'),  # Dense layer expanding to 32 units
    layers.Dense(input_dim, activation='linear')  # Output layer with the original dimension and linear activation
])

# Combine encoder and decoder into the autoencoder model
autoencoder = models.Sequential([encoder, decoder])

# Compile the autoencoder model with optimizer, loss function, and evaluation metric
autoencoder.compile(optimizer='adam', loss='mean_squared_error')

# Train the autoencoder to reconstruct the input data (X_train)
# We train it to learn the normal (non-fraudulent) patterns
autoencoder.fit(X_train, X_train, epochs=10, batch_size=32, shuffle=True, validation_data=(X_test, X_test))

# Detect anomalies by calculating the Mean Squared Error (MSE) between original and reconstructed data
y_pred = autoencoder.predict(X_test)
mse = np.mean(np.power(X_test - y_pred, 2), axis=1)  # MSE for each sample

# Visualize the reconstruction error distribution to help determine a threshold for anomaly detection
plt.figure(figsize=(10, 6))
plt.hist(mse, bins=50, alpha=0.5, color='b', label='Reconstruction Error')
plt.xlabel("Reconstruction Error")
plt.ylabel("Frequency")
plt.legend()
plt.title("Reconstruction Error Distribution")
plt.show()

# Define thresholds for tuning (can adjust based on distribution)
thresholds = np.arange(0.1, 1.0, 0.1)

# Iterate through different threshold values to identify potential anomalies
for threshold in thresholds:
    anomalies = mse > threshold  # Mark samples with MSE above threshold as anomalies

    # Count the number of anomalies detected for the current threshold
    num_anomalies = np.sum(anomalies)
    print(f"Threshold: {threshold:.1f}, Number of anomalies: {num_anomalies}")

# Evaluate the model performance on the test data at the last threshold value used
print("Confusion Matrix:")
print(confusion_matrix(y_test, anomalies))

print("\nClassification Report:")
print(classification_report(y_test, anomalies))

# Plot the confusion matrix for visual evaluation
plt.figure(figsize=(6, 4.75))
sns.heatmap(confusion_matrix(y_test, anomalies), annot=True, annot_kws={"size": 16}, fmt='d')
plt.xticks([0.5, 1.5], labels=["Normal", "Anomaly"], rotation='horizontal')
plt.yticks([0.5, 1.5], labels=["Normal", "Anomaly"], rotation='horizontal')
plt.xlabel("Predicted label", fontsize=14)
plt.ylabel("True label", fontsize=14)
plt.title("Confusion Matrix", fontsize=14)
plt.grid(False)
plt.show()
