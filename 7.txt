# Import necessary libraries
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np

# Define directory paths for training and test data
train_dir = "cifar-10-img/train"
test_dir = "cifar-10-img/test"

# Data generators for training and testing sets with rescaling
train_datagen = ImageDataGenerator(rescale=1.0 / 255)
test_datagen = ImageDataGenerator(rescale=1.0 / 255)

# Define batch sizes
train_batch_size = 5000
test_batch_size = 1000

# Load training and test data using the generators
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(32, 32),
    batch_size=train_batch_size,
    class_mode='categorical'
)
test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(32, 32),
    batch_size=test_batch_size,
    class_mode='categorical'
)

# Retrieve training and testing data batches
x_train, y_train = train_generator[0]
x_test, y_test = test_generator[0]

# Confirm data loading
print("Training samples:", len(x_train))
print("Testing samples:", len(x_test))

# Load the VGG16 model without top layers for transfer learning
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))

# Freeze base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom classifier layers
x = Flatten()(base_model.output)
x = Dense(256, activation='relu')(x)
x = Dropout(0.3)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.3)(x)
predictions = Dense(10, activation='softmax')(x)

# Create the model
model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model with optimizer, loss, and evaluation metrics
model.compile(optimizer="adam", loss='categorical_crossentropy', metrics=['accuracy'])

# Train the classifier layers
model.fit(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_test, y_test))

# Fine-tuning: unfreeze last few layers of the base model
for layer in base_model.layers[-4:]:
    layer.trainable = True

# Rebuild the model with additional fine-tuned layers
x = Flatten()(base_model.output)
x = Dense(256, activation='relu')(x)
x = Dropout(0.3)(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.3)(x)
predictions = Dense(10, activation='softmax')(x)

# Recreate and compile the model with adjusted parameters
model = Model(inputs=base_model.input, outputs=predictions)
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the fine-tuned model
model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))

# Make predictions on test set
predicted_values = model.predict(x_test)

# Retrieve class labels
labels = list(test_generator.class_indices.keys())

# Display sample prediction and actual label
n = 890  # Index of the sample image
plt.imshow(x_test[n])
plt.title(f"Predicted: {labels[np.argmax(predicted_values[n])]}, Actual: {labels[np.argmax(y_test[n])]}")
plt.show()
